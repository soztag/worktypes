# BIBB/BAuA

We explore this hypothesis using fine-grained data on the work activities performed by 20.036 working-age (>= 15 years) persons employed for more than 10 hours per week from Germany, gathered as part the regular [BIBB/BAuA Employment Survey](https://www.baua.de/DE/Themen/Arbeitswelt-und-Arbeitsschutz-im-Wandel/Arbeitsweltberichterstattung/Arbeitsbedingungen/BIBB-BAuA-2012.html) in 2012 [@hall_bibb/baua_2015; @wittig_grundauswertung_2012].
Respondents independently rated the below activities in table \@ref(tab:act-vars) as occuring
*often*, *sometimes* or *never* in their jobs. [^no-computers] [^translation] [^details]

[^no-computers]: The full list of types of work performed also includes *working with computers* (`F318`) and *using the Internet or editing e-mails* (`F319`).
We do not include these in our analysis, because they are oddly unspecific and refer to the *tool* used, rather than the *type* of work conducted.
These are *orthogonal* to the other activities; you may, for example, *provide advice and information* (`F314`) *by* writing an email.

[^translation]: The english translation of items is from @rohrbach-schmidt_bibb/baua_2013: 28f, the german original and id are from @hall_bibb/baua-erwerbstatigenbefragung_2011.
In addition, we created short names for more intuitive handling in the analysis.

[^details]: The full german wording of the interviewer prompt reads:

    > "Denken Sie bitte an ihre Berufstätigkeit als [...].
    > Ich nenne Ihnen nun einige ausgewählte Tätigkeiten.
    > Sagen Sie mir bitte, wie häufig diese bei Ihrer Arbeit vorkommen, ob häufig, manchmal oder nie."

    The order of response options from `F303` to `F320` was randomized.

    `F321` (logical) allowed for additional, open-ended activities, recorded in `TF321` (string), but this variable is not publicly available.

```{r act-vars}
act_vars <- tibble::tribble(
  ~short,
  ~id,
  ~english,
  ~german,

  "manuf",
  "F303",
  "Manufacturing, producing goods and commodities",
  "Herstellen, Produzieren von Waren und Gütern",

  "meas",
  "F304",
  "Measuring, testing, quality control",
  "Messen, Prüfen, Qualität kontrollieren",

  "monit",
  "F305",
  "Monitoring, control of machines, plants, technical processes",
  "Überwachen, Steuern von Maschinen, Anlagen, technischen Prozessen",

  "repair",
  "F306",
  "Repairing, refurbishing",
  "Reparieren, Instandsetzen",

  "purchase",
  "F307",
  "Purchasing, producing, selling",
  "Einkaufen, Beschaffen, Verkaufen",

  "transp",
  "F308",
  "Transporting, storing, shipping",
  "Transportieren, Lagern, Versenden",

  "marketing",
  "F309",
  "Advertising, marketing, public relations",
  "Werben, Marketing, Öffentlichkeitsarbeit, PR",

  "org",
  "F310",
  "Organising, planning and preparing work processes. Here we are not talking about your own work processes.",
  "Organisieren, Planen und Vorbereiten von Arbeitsprozesse. Gemeint sind hier nicht die eigenen Arbeitsprozesse.",

  "dev",
  "F311",
  "Developing, researching, constructing",
  "Entwickeln, Forschen, Konstruieren",

  "train",
  "F312",
  "Training, instruction, teaching, educating",
  "Ausbilden, Lehren, Unterrichten, Erziehen",

  "info",
  "F313",
  "Gathering information, investigating, documenting",
  "Informationen sammeln, Recherchieren, Dokumentieren",

  "advice",
  "F314",
  "Providing advice and information",
  "Beraten und Informieren",

  "ent",
  "F315",
  "Entertaining, accomodating, preparing food",
  "Bewirten, Beherbergen, Speisen bereiten",

  "care",
  "F316",
  "Nursing, caring, healing",
  "Pflegen, Betreuen, Heilen",

  "protect",
  "F317",
  "Protecting, guarding, patrolling, directing traffic",
  "Sichern, Schützen, Bewachen, Überwachen, Verkehr regeln",

  "clean",
  "F320",
  "Cleaning, removing waste, recycling",
  "Reinigen, Abfall beseitigen, Recyclen",

  "open",
  "F321",
  "Did we forget an activity that you frequently perform?",
  "Haben wir eine Tätigkeit vergessen, die Sie häufig ausüben?",

  "open_activity",
  "TF321",
  "Open-ended description of missing activity.",
  "Offene Beschreibung der fehlenden Tätigkeit."
)
kable(
  x = act_vars,
  caption = "Occupational Activity Variables from BIBB/BAuA 2012",
  format.args = list(
    justify = "right"
  ))
# TODO make cols 1, 2 monospace
# TODO fix wrapping issue in latex
# pander is not a good fix, because it does not support labels
# kableExtra only works for html and latex, both of which must be pre-specified
```


## Limitations of the Data

The BIBB/BAuA Employment Survey is a relatively large, in-depth and widely used dataset, going back to a a first wave in 1979 (using a longitudinal, but not panel design).
It also carries some limitations that are worth keeping in mind for the below analysis:

1. The survey may carry substantial (hidden) bias.

    The universe of cases are members of *private* households with *answered landline phones* only (for random-digit dialing) [@rohrbach-schmidt_bibb/baua_2013: 5].
    **Employees without landline phones**, estimated to be 11% already in 2008 [as cited in @gensicke_bibb/baua-erwerbstatigenbefragung_2012: 12] are not part of the sampling universe, and thus not even included in the response rate.
    It is hard to even estimate the bias that might result from this design choice, but the polling firm conducting BIBB/BAuA 2012 suggests that mobile-only persons have lower income and education, and are more likely to be marginally employed and live in east Germany.

    A ***design* weighting model** (`Des2012`) was used to approximate 2011 microcensus on selected sociodemographic variables (gainful activity, vocational position, West/East, Federal state, education, gender, martital status) [@rohrbach-schmidt_bibb/baua_2013: 7].

    This universe of cases yielded a response rate of 44.3% with completed interviews [@rohrbach-schmidt_bibb/baua_2013: 6], under-representing middle and lower education levels by 5% each at one point [@gensicke_bibb/baua-erwerbstatigenbefragung_2012: 18].

    To counteract this non-response bias, a de-facto **dynamic quota re-sampling** was introduced, and interviews with too highly educated would-be interviewees were aborted during the latter part of the fieldwork.

    In addition to the design weights, and the dynamic quota sampling, a **structural weighting model** was produced to further counteract non-response bias (`Gew2012`), using the same reference data and variables as in the above `Des2012`.

    Multidimensional weighting models are inherently complex and yet imperfectly understood.
    The combined chosen design and structural weighting are quite substantial here, with an efficiency of 59 (100 would indicate a perfect pre-weighting sample) and a standard deviation of weights of 0.830, and this *in spite* of the aforementioned partial dynamic quota sampling.

    All of these limitations and counteractions may interact in complex and unforeseen ways, and, together, give rise to substantial hidden biases, especially on unobserved or latent variables.

    Furthermore, the strong reliance on weights causes problems for a few of the multivariate analyses considered in the below.

2. The activities data are quite coarsely scaled with only three levels (`often`, `sometimes` or `never`).
    It may be questionable to treat these as interval-scaled data, and they should rather be considered ordinal.

    As a result, the information-theoretical value of this data is quite limited, and may imply relatively steep cutoffs in parallel analyses in the below: with such few levels, depending on the mode of analyses, some observed pattern may well be indistinguishable from random chance.

3. The activities data are each surveyed *independently* and as a *normative*, not *ipsative* measure.
    We cannot, stricly speaking, infer from the data how much of their given work time participants spent on each of these activities, *relatively*.
    The existing data are emphatically *not* work-diary entries, that would give us a clear view on how respondents spent their work time.

    For example, it is conceivable that a participant rates, say, `repair` work as `often`, and all other work as `none`, because repair work is in fact *all* that she does in her work time.
    Another respondent may also rate `repair` work as `often`, but *also* `meas`uring work, because she does both for about half of her time.

    In such a simple case, where we assume the activies (and reports thereof) to be mutually exclusive, we *may*, if crudely so, impute diary-type records by simply scaling *all* the rated activities per person to their known total number of work hours.

    However, such an imputation fails, if the activities are not all mutually exclusive.
    This is evidently the case for the the computer-related activies `F318` and `F319`, which refer to *tools* with which *many* of the othr activities may be conducted:
    For example, one may gather `info`rmation, *by* using a computer (`F318`).
    We have, for this reason, excluded this activity from the following analysis.

    Unfortunately, a similar, non-mutually exclusive relationship is conceivable for other activities as well.
    For example, a worker may respond `often` to *both* `monit`oring *and* `repair*, because the two activities often or always overlap.

    The more the activies may overlap in the workplace or recollection of the respondent, the less we can consider this data as ipsative, diary-type measurements, which strictly limits the type of analysis possible.

    There is, moreover, no apparent way to test to what degree activities *are* considered to be mutually exclusive or not by respondents, so any methodological decision will entail a substantial risk of garbage-in-garbage-out (GIGA).

    An explicitly ipsative survey form, where respondents had *ranked* the activities, or even allocated a time budget to the separate activities would be vastly more useful for our purposes, and more unambiguous in its appropriate use (see [this issue](https://github.com/QWrks/worktypes/issues/3)).

4. The longitudinal design of the BIBB/BAuA surveys strictly limits our ability to investigate any trends in activities, because we cannot match individual respondents (see [this issue](https://github.com/QWrks/worktypes/issues/4)).
    As a stopgap, we can only try to match units at the macro level, such as industries or regions, though this would strictly speaking constitute an ecological fallacy, and may be further beset by changing sector compositions and definitions over the span of the BIBB/BAuA survey waves (see [this issue](https://github.com/QWrks/worktypes/issues/5)).

5. The BIBB/BAuA data, by definition, over cover workplace activities, and not unpaid or informal work.

    Some of the above-hypothesized Marxist typology of activities often take place as informal or unpaid work, such as the reproduction of use values in raising a child, or caring for the elderly.
    By contrast, the production of exchange values, by definition, almost exclusively happens in the context of the formal economy, because otherwise, property rights to those very exchange values could not be instituted (see [this issue](https://github.com/QWrks/worktypes/issues/9)).

    There is, therefore, a mismatch between the BIBB/BAuA data and the typology it is supposed to test.
    As long as we limit our research question to *relative changes in emphasis* between the cells of this marxist typology in the workplace, this need not be a problem: reproduction of use values, for example, may be generally underrepresented in the workplace, but *changes* should still be detectible.

    However, an additional problem may arise as, for example, more reproduction of use values is transferred from unpaid and informal work, to formal work, as appears to be the case in the extension of child care.
    Such a transfer into the formal economy would increase the relative share of this marxist category, and may thereby mask other underlying changes in the economy.

    For example, the influx of (paid, formal!) work hours in recently extended child care may partially mask in the relative activity data a trend to more exchange-value oriented activities *all other things equal*.

    In short, the problem with workplace activity data is that all other things tend emphatically *not* to be equal, as the boundaries over what is, and is not formal and paid work tend to shift over time, especially the long periods of time covered by BIBB/BAuA.



## Descriptives

```{r data-import, include=FALSE, eval=FALSE, cache=FALSE}
bba <- haven::read_dta(file = "../../Google Drive/Hohenheim RA/bibbbaua/BibbBaua_SAP-mit-AV.dta")

# import activities
act <- bba[act_vars$id[1:16]]
act <- haven::as_factor(x = act, levels = "labels")
act <- purrr::map_df(.x = act, .f = function(x) {
    x[x == "Keine Angabe"] <- NA
    x <- forcats::fct_recode(f = x,
                        often = "Häufig",
                        sometimes = "Manchmal",
                        never = "Nie",
                        NULL = "Keine Angabe")
    x <- forcats::fct_relevel(f = x,
                              "never",
                              "sometimes",
                              "often")
    x <- as.ordered(x)
    # x <- forcats::fct_explicit_na(f = x, na_level = "no response")
    return(x)
  })
colnames(act) <- act_vars$short[1:16]

# add weights
act$weight <- bba$Gew2012

# add weekly hours
act$hours <- as.integer(bba$AZ)
act$respondent <- as.character(bba$Intnr)
saveRDS(object = act, file = "act.rds")
```

```{r read-from-disc}
act <- readRDS(file = "act.rds")
```


Let us first consider the weighted and unweighted frequencies of the different activities.

```{r freqs, fig.cap="Raw and Weighted Frequencies of Work Activities"}
freqs <- gather(data = act, key = "activity", value = "frequency", manuf:clean)
freqs <- group_by(.data = freqs, activity, frequency)
freqs <- summarize(.data = freqs, count = n(), wcount = sum(weight))
freqs <- mutate(.data = freqs,
                raw = count/sum(count),
                weighted = wcount/sum(wcount))
freqs <- gather(data = freqs,
                raw,
                weighted,
                -activity,
                -frequency,
                key = weighting,
                value = percentage)
# reinstate factors (killed by above)
freqs$frequency <- readr::parse_factor(x = freqs$frequency, levels = c("never", "sometimes", "often"), ordered = TRUE, na = "no reply")

g <- ggplot(data = freqs, mapping = aes(
  x = weighting,
  y = percentage * 100,
  fill = activity,
  alpha = frequency))
g <- g + geom_bar(stat = "identity")
g <- g + facet_wrap(~activity)
g <- g + scale_alpha_manual(
  values = c(never = 0, sometimes = 0.5, often = 1),
  name = "Frequency",
  na.value = 1)
g <- g + scale_fill_discrete(name = "Activity", guide = FALSE)
g <- g + scale_y_continuous(name = "Percentage")
g <- g + scale_x_discrete(name = "Weighting")
if (!(pensieve:::assert_n_infer_use_js())) {
  plot(g)
} else {
  g <- g + theme(legend.position = "none")
  ggplotly(g)
}
```

Figure \@ref(fig:freqs) plots the raw and weighted counts of reported frequencies across the reported work activities.
The activities have a fairly wide spread; giving `advice` is done often by more than half of the respondents, while `manuf`acturing is quite rare.
The overall reported frequencies sum up to *much* more than 100 percent, which raises the possibility that participants did not consider the activities to be mutually exclusive.
It also shows that, at least at the aggregate level, the weighting has some effect, but does not cause a dramatic shift in the relative frequency of activities, which may

```{r freq-hours, fig.cap="Reported Working Hours (Unweighted)"}
g <- ggplot(data = act, mapping = aes(x = hours)) + geom_histogram(na.rm = TRUE, bins = 30)
if (pensieve:::assert_n_infer_use_js()) {
  ggplotly(g)
} else {
  plot(g)
}
```

Figure \@ref(fig:freq-hours) displays the distribution of reported working hours, which we will be using in some of the below calculations.


## Analytical Decisions

To match the activity data from BIBB/BAuA to the hypothesized Marxist typology of labor, we must first test whether there are, in fact, any patterns in the reported activity, and if so, what those pattern are, and how well they might correspond to our theoretical assumptions.

There are a (large) number of ways to accomplish such a data reduction, and to choose the appropriate approach for the research question and data at hand, we must first work through a list of decision criteria.


### Unit of Analysis

We here preliminarily assume an *individual* unit of analysis, with any pattern or change in activities measured at the level of individual jobs or respondents.
This offers the greatest resolution, but because BIBB/BAuA is a longitudinal, not a panel survey, we cannot match or compare individuals between the waves, which will complicate any comparisons over time.
Using a higher-level unit of analysis might help us match sectors or industries between survey waves, and more clearly identify trends, though such aggregation may cause its own problems (see [this issue](https://github.com/QWrks/worktypes/issues/5)).


### Ordinal vs. Interval/Ratio Scale of Measurement

As discussed in the above, the activity ratings `never`, `sometimes` and `often` are **ordinally** scaled, not intervally scaled, dramatically limiting the appropriate statistical procedures.
However, the *latent concept* beind this rather crude measure is clearly interval (even ratio-scaled) in nature: how much time you spend on any given activity exists as an objective reality, and respondents will have substantial experience reporting these times in ratio form ("This task will take about 30 minutes" or "I spent 4 hours doing this").
We may even assume that respondents held approximate, but ratio-scaled representations of their workday activity schedules, but *binned* these to respond to the BIBB/BAuA questionnaire.

Fortunately, there exists a statistical procedure to *deduce* -- as far as possible -- the underlying ratio-scaled latent variable behind their ordinal manifestations.
Polychoric correlations do this by estimating correlation coefficients based on deduced underlying, normally-distributed ratio-scaled variables.
The resulting correlation (or covariance) matrices are then amenable to ordinary factor analyses or PCA, opening up a familar and frequently used set of statistical tools.

There also exist approaches that "natively" support ordinal (and categorical) data, including item response theory (IRT, and other structural equation modeling), polynomial or nonlinear techniques of dimensionality reduction, though these are far more complex and less well understood (see [this issue](https://github.com/QWrks/worktypes/issues/11)).


### Latent Variable Model vs. Dimensionality Reduction

Our present research question assumes that there are structural forces in advanced capitalism that give rise to particular patterns of work activities, such as the above-hypothesized Marxist of labor.
We have, therefore, subscribed to a latent variable model of our data, where the BIBB/BAuA-variables are merely *manifestations* of underlying *latent* concepts, such as, for instance, the share of reproductive activities.

This need not imply a hypothetico-deductive approach, but is also compatible with the exploratory stage of our present work:
We may not know precisely *what* the latent concepts are, but we assume ex-ante that there *are* such latent patterns in the data.

A latent variable model, and the factor analytic implementations which are here relevant, conceive of individual records (be they cases or variables) as the manifest product of the latent variables *and* an ideosyncratic error term (or its inverse, the communality).
All latent variable procedures seek to minimize this error term, but allow that some records cannot be fully explained by the resulting model.

This is a fitting procedure for our research question:
it seems plausible that in addition to the structural forces of interest, that there will be individual and ideosyncratic patterns of work activities, which need not concern us.

A latent variable model stands in contrast with approaches for simple dimensionality reduction, such as PCA, where *no* error is allowed, but *all* of the variance must be explained, as far as possible.
While such a a more stringent approach is ultimately inadequate for our research question, we may still resort to simple dimensionality reduction, which is often more robust and easier to carry out.


### Segmentation vs. Factoring

Cluster analyses is a family of algorithms to *segment* cases (or variables) into discrete groups, whereas factor analysis *factorises* cases (or variables) so that their (common) variance can be explained using fewer, continuous variables.
Consequently, a cluster analysis is warranted when the latent phenomenon is assumed to be categorical, as, for example, biological species.
By contrast, a continous latent construct, such as "extroversion", may be better analysed using factor analysis.

While the above-hypothesized typology is *categorical*, we have no reason to assume that jobs would fall into clear-cut, dichotomous patterns of work activities.
The typology is one of ideal types, easily presented in categorical form, but this does not imply that ontologically, *only* such ideal types would exist.
Indeed, it seems more plausible that any given actual job might be a continuous mixture of these ideal-types, and a factor analytic approach is therefore more appropriate to our research question.

While it is often erroneously assumed, that cluster analysis is suitable (only) for grouping cases, and factor analysis (only) for grouping variables, but while this is often the case, it need not be so.
We can segment or factorise both cases *and* variables.

Cluster analyses can also appear to more readily highlight authentic and representative cases (or variables), without the need for factor rotation and scoring, but this is also a misunderstanding.
Clustering, per se, is *not* a well-defined goal, and the (vast) choice of concrete clustering algorithms imply quite different criteria what it means to "cluster", raising similar -- if more opaque -- problems as factor rotation.


### Mode of Analysis (Q vs R)

Independent of the *unit* of analysis, we can also choose a *mode* of analysis.  [^alphabet-soup]

[^alphabet-soup]: The terminology is taken from @Thompson-2004 [82].

*R-technique* (or R-mode) refers to an analysis where participants (here, loosely, job holders) constitute the rows, and variables (here, loosely, ordinal frequency of work activities) constitute the columns.
By contrast, in *Q-technique*, rows are populated by variables, and columns by participants.
This approach is also known, more informatively, as *inverted factor analysis*, because, relative to R-technique, the rows and columns in the data matrix are being *transposed* prior to analysis.

To illustrate the difference between these two modes, consider an analysis of the lengths of different body parts of different mammals (example taken from @Brown1980: 13).
In R-mode, the animals are the observations-rows in the data matrix, and the body parts are the variable-columns, with lengths entered in, say, centimeters, in the cells.
R-analysis then reveals common patterns among the body part lengths, *across* the animals. [^stand-r]
For example, across all mammals, leg and arm lengths probably covary more strongly (because they need to be approximately the same length for four-legged mammals) than leg length and torso length.
R-mode, in other words, yields the dominant dimensions across which mammals *differ*, such as
In Q-mode, the data matrix is transposed, with animals as the variable-columns and body parts as the observation-rows.
Q-analysis then reveals common patterns among the animals, *across* their body parts.
For example, the body part measurements of a gorilla and a human will be more similar, than the body part lengths of a giraffe. [^stand-q]
Q-mode, in, other words, yields *species or gestalt factors*, such as "ape-like" and "giraffe-like"

[^stand-r]: Notice that because the body part lengths (in columns) are centered (for covariance-based analyses) and standardized (for correlation-based analyses), it is *not* the absolute similarity in body part lengths that matter, but their covariance *relative* to that body parts mean.
    For example, eye and mouth diameters need not be more strongly related than leg and arm lengths, even though the former are generally smaller in all mammals.
    Instead, what matters is whether the *differences* from the across-species eye and mouth diameters covary with, say, the differences from the across-species leg lenths.

[^stand-q]: Here too, columns are centered (and standardized) before factorisation, now *across* body parts.
    For example, the eye and mouth diamaters, as well as leg lenths of humans, gorillas and giraffes will be re-calculated as differences from the *mean* of all measured body part lengths for that animal.
    As a result, the difference in overall size between humans, gorillas and giraffes is centered away, and only the *proportions* remain to covary.

While Q and R-mode are starkly different analytically, they are quite similar mathematically.
At least when relying on simple procedures such as Principal Components Analysis, Q- or R-mode are merely semantic differences.
The data matrix is always linearly decomposed into two smaller matrices, with one giving the association between factors and rows (the "scores"), and one giving the association between the factors and the columns (the "loadings").
Thus, the scores matrix in R-mode is the loadings matrix in Q-mode, and vice versa.

In terms of the mathematics, Q- and R-mode only differ substantially, when automatic rotation procedures are being used upon extraction, as is frequently the case to make the factors (or components) more readily interpretable.
Using the above example, R-mode rotation would target the decomposed matrix giving factors and body parts, while in Q-mode, rotation would target the matrix giving factors and animals.
In R, we would rotate to render interpretable the patterns among the body parts, and in Q-mode the patterns among the animals.
In such a simplified example, R- and Q-mode are simply alternative ways of *looking* at *the same* patterns, presented with a different focus.

Unfortunately, this convenient equivalence of R- and Q-mode only holds, when the unit of measurement is objectively the same accross all observations and variables (cm for body parts), and centering and standardization are meaningful both by columns *and* by rows.
Alas, this is seldom the case with real-world datasets in the social sciences, and we must therefore carefully consider whether R- and/or Q-mode analyses are permissible.

Let us first consider an R-mode analysis of the BIBB/BAuA work activity data, with the activities as variable-columns and the respondents as observation-rows.
Strictly speaking, we cannot assume precise equality of measurement units across rows, because some respondents may generally overestimate the ordinal rating of *all* activities, while others underestimate (with only three levels, spread is less of a concern).
Such scale response biases are a widespread concern, but cannot easily be avoided in normative measures, and are generally ignored.
Centering by columns of work activity, as implied by R-mode analysis, is less problematic:
Recalculating each activity frequency as the difference from its *mean* makes intuitive sense: if almost *everyone* does `some`, say, `org`anising, we are especially interested in those relatively few who do it `never` or `often`, and what other activities covary with their extreme `org`anising.
Conversely, if an activity is reported very rarely, even a `sometimes` rating, might be quite informative.

While centering around the column means is obligatory, we *can* retain the differences in spread (standard deviation), by relying only on the covariance matrix.
For example, if there is an activity that is either done `never` or `often` by almost all respondents, but not `sometimes`, retaining this relatively greater spread will give this variable and any resulting covariance a greater impact on the overall factorisation.
By contrast, activities which have mostly `sometimes` (or any other) rating will have a smaller spread and any related covariances would yield a smaller impact.

Retaining the spread, and relying on covariances, rather than correlations, would, in effect, place greater emphasis on activities which are more *discriminant*, loosely speaking.
Our research question implies no special emphasis on discriminant activities, such as `manuf`acturing versus more widely-spread activities such as `org`anisation (see \@ref(fig:freqs)).
In fact, we are especially interested in patterns regarding the miscellaneous activities, such as `train`ing and `marketing`.
As a result, we will rely on the correlation matrices in the below R-mode analysis.

While an R-mode analysis of the data appears permissible, it will produce a result slightly off the mark of our research question:
We may learn how activities are related to one another, but this view will not reveal ideal-types of organic, actual existing jobs with reported activies.
R-mode here, as always, produces *analytical*, as opposed to *holistic* results: we learn which activities covary, *relative* to the mean and spread of the entire sample (of jobs), negating that which describes any *one* such job.

This makes R-mode analysis somewhat ill-suited for our research question, and would appear to make Q-mode a better choice.

In our preliminary operationalisation of the above Marxist typology of labor, we assume that the cells in table \@ref(tab:examples) map onto particular "jobs", each of which may, by its distribution of activities, score on these ideal types.
Many jobs will likely feature *several* of the above types of labor, such as when an engineer is also involved with marketing aspects of the final product.
However, it seems plausible to assume that at the *individual level*, in all but the smallest organisations, some inter-personal division of labor will demarcate distinct roles along this categorisation, with one *dominant* perspective. [^functional-org]

[^functional-org]: Such inter-personal division of labor by labor type is akin to a functional grouping at the organizational level.
    We assume that even should an organisation adhere to a different structure --- say, an org chart grouped by region or product --- *individual* jobs could still be characterized in our above "functional" terms.

By this operationalisation, we would expect to find *clusters of jobs* who share yet-to-be-determined characteristics that reflect the above categorisation.
For example, there should be a group of otherwise potentially dissimilar jobs, which are all concerned with designing, maintaining and enforcing the capitalist mode of production (the south-eastern cell in \@ref(tab:examples)), including, perhaps, lobbying, legal work but also security and defense.

In Q-mode *jobs* constitute the *columns*, and characteristics (or their manifest proxies, the activities) constitute the *rows* of our would-be data.
We are, in other words, investigating whether *jobs covary across job characteristics*, that is, whether some jobs are similar *as measured by these characteristics*.

By contrast, in R-mode we would be looking for *covarying job characteristics across jobs*, that is, whether some characteristics co-occured on some surveyed jobs.

There is an extreme, hypothetical scenario in which R- and Q-mode would yield substantially similar results:
when *all* activities highly load on one of a few factors each.
This would imply that jobs are completely distinct, with no overlap in activities.
In this scenario, R-mode would yield these jobs as a few, highly loading factors, and Q would reveal them as completely different jobs, each scoring highly (or not at all) on all the variables.

This is, alas, an unlikely scenario, and one out of sync with our research question.
If there are, in fact, latent, Marxist ideal-types of labor in late capitalism, these are likely to be quite muddied, and they are likely to be superseded by higher-loading other factors.
The answer to our research question, in other words, is to be found in the nuances of covarying jobs.

More tentatively, we would also assume that *if* there is a large shift of Marxist labor under way, we would expect this shift to occur *within* jobs, rather than for it to materialize as an entirely, entirely distinct class of jobs.

Analysing the BIBB/BAuA-activity data in Q-mode is, alas, problematic.
The problems begin with centering the Q-mode observation-column of all respondents over all their respective activities.
After centering, *all* respondents would have the same *net* activity ratings, distributed proportionally over their `sometimes` (say, `= 1`) and `often` (`= 2`) activities. [^polychor-q]
For example, someone who only reported `often` for `manuf`acturing, and `none` for all other 15 activities would receive a `.9375` for `manuf`acturing, and a `-.0625` for all others, summing to `0`.
By contrast, a participant who reported `often` for eight categories, and `none` for the remaining 8, would receive `.5` for often-rated activies, and `-.5` for none.

[^polychor-q]: Using the aforementioned polychoric correlations would infer the underlying interval-scaled data in a more sophisticated way, as described in the above.
    The implications would remain the same.

```{r centered-activities, fig.cap="Centered Activities"}
diaries <- as.matrix(act[,1:16])
diaries[,] <- NA
storage.mode(diaries) <- "numeric"
diaries_12 <- diaries
for (i in 1:nrow(act)) {
  thiscase <- unlist(act[i,1:16])
  thiscase <- as.character(thiscase)
  thisfreq <- table(thiscase)
  never <- 0
  atalls <- as.integer(thisfreq["sometimes"] + (thisfreq["often"] * 2))
  sometimes <- 1/atalls
  often <- 2/atalls
  
  for (p in 1:length(thiscase)) {
    diaries[i,p] <- switch(EXPR = as.character(thiscase[p]),
                           "often" = often,
                           "sometimes" = sometimes,
                           "never" = never,
                           `NA` = NA)
    diaries_12[i,p] <- switch(EXPR = as.character(thiscase[p]),
                           "often" = 2,
                           "sometimes" = 1,
                           "never" = 0,
                           `NA` = NA)
  }
}
diaries <- cbind(
  as.tibble(diaries),
  act[ ,17:19])

diaries_long <- gather(data = diaries, key = "activity", value = "percent", manuf:clean)
diaries_long$raw <- gather(
  data = as.tibble(diaries_12), 
  key = "activity", 
  value = "raw")$raw

diaries_long <- group_by(.data = diaries_long,
                         respondent)
diaries_long <- mutate(
  .data = diaries_long,
  centered = raw - mean(raw, na.rm = TRUE),
  scaled = scale(x = raw, center = TRUE, scale = TRUE))

diaries_sample <- na.omit(object = diaries_long)
chosen <- as.character(sample(x = diaries_sample$respondent, 
                              size = 20, 
                              replace = FALSE, 
                              prob = diaries_sample$weight))
diaries_sample <- dplyr::filter(.data = diaries_sample,
                                respondent %in% chosen)

g <- ggplot(data = diaries_sample, mapping = aes(
  x = respondent,
  y = centered,
  fill = activity))
g <- g + geom_bar(stat = "identity", position = "dodge")
g <- g + scale_y_continuous(name = "Centered Activities (Often = 2, Sometimes = 1)")
g <- g + scale_x_discrete(name = "Selected Respondents (BIBB/BAuA Intnr)")
g <- g + scale_fill_discrete(name = "Activity")
g <- g + theme(axis.text.x = element_text(angle = 90, hjust = 1))
if (!(pensieve:::assert_n_infer_use_js())) {
  plot(g)
} else {
  ggplotly(g)
}
```

Figure \@ref(fig:centered-activities) illustrates this centering operation for some randomly selected participants.
Surprisingly, *all* respondents now have values for *all* activities, even those they rated as `never`.
This is because their activities are now expressed *relative* to their mean of `0`.
A positive value indicates that a respondent undertakes an activity *more often* than that respondent undertakes an activity *on average*.
This appears counterintuitive, but is necessary to calculate covariances:
Respondents are similar, when they share actitivies with extreme positive or negative values.

After centering only, those participants with greatly spread values may have an outsized effect on the resulting factors.

```{r scaled-activities, fig.cap="Standardized Activities"}
g <- ggplot(data = diaries_sample, mapping = aes(
  x = respondent,
  y = scaled,
  fill = activity))
g <- g + geom_bar(stat = "identity", position = "dodge")
g <- g + scale_y_continuous(name = "Centered and Standardized Activities (Often = 2, Sometimes = 1)")
g <- g + scale_x_discrete(name = "Selected Respondents (BIBB/BAuA Intnr)")
g <- g + scale_fill_discrete(name = "Activity")
g <- g + theme(axis.text.x = element_text(angle = 90, hjust = 1))
if (!(pensieve:::assert_n_infer_use_js())) {
  plot(g)
} else {
  ggplotly(g)
}
```

Figure \@ref(fig:scaled-activities) gives such scaled data.

The negative values are, in a way, the smallest problem with this scaled data, because they stem from the rather crude treatment of the `never` response.

The greater problem with Q-scaled data, is that the respondents were not aware that their replies would be standardized in this way *across different questions*.

In effect, we would be treating their responses *as if they had* replied to an ipsative instrument, and crudely imputing diaries from normative data.

```{r imputed-diary, fig.cap="Select Imputed Activity Diaries"}
g <- ggplot(data = diaries_sample, mapping = aes(
  x = respondent,
  y = percent * 100,
  fill = activity))
g <- g + geom_bar(stat = "identity")
g <- g + scale_y_continuous(name = "Imputed Percentage of Worktime")
g <- g + scale_x_discrete(name = "Selected Respondents (BIBB/BAuA Intnr)")
g <- g + scale_fill_discrete(name = "Activity")
g <- g + coord_flip()
if (!(pensieve:::assert_n_infer_use_js())) {
  plot(g)
} else {
  ggplotly(g)
}
```

```{r imputed-diary-byhrs, fig.cap="Select Imputed Activity Diaries, Scaled to Weekly Hours"}
g <- ggplot(data = diaries_sample, mapping = aes(
  x = respondent,
  y = percent * hours,
  fill = activity))
g <- g + geom_bar(stat = "identity")
g <- g + scale_y_continuous(name = "Imputed Weekly Hours Spent on Activity")
g <- g + scale_x_discrete(name = "Selected Respondents (BIBB/BAuA Intnr)")
g <- g + scale_fill_discrete(name = "Activity")
g <- g + coord_flip()
if (!(pensieve:::assert_n_infer_use_js())) {
  plot(g)
} else {
  ggplotly(g)
}
```

Figures \@ref(fig:imputed-diary) and \@ref(fig:imputed-diary-byhrs) give such imputed diary entries, scaled to a sum of 100 percent and to their reported weekly working hours respectively.

This presentation of the data looks more palatable, but it carries the same underlying problems of normative data: 

1. We do not know whether understood the response options in the same way.
    On the one hand, some participants might have had a lower threshold for `often` than others, diluting the imputed time spent on each activity.
    On the other hand, participants who respond `often` to more activities really *might* have a more diverse workday.
    The problem is, that, with normative data, we don't know which is the case; centering and standardisation will negate mean and spread either way.
2. It is also conceivable that given the *ipsative* nature of time spent on a number of competing activities, respondents *may* already have "self-censored" in their usage of the normative survey instrument, giving, in effect, ipsative replies of their time budgets.
    They may also *not* have done that, and replied to each activity in isolation; there is now way of knowing from the data.
3. We do not know whether participants considered the responses to the various activity questions to be mutually exclusive, or not.
    On the one hand, they might have assumed that these activies can *overlap* during any one time in their workday --- perhaps, they replied `often` to `repair` and `monit`oring, because they often do both at the same time.
    On the other hand, they might have assumed that these activies must *not* overlap, and consequently rated `repair` and `monit`oring both as merely `sometimes`.

```{r distro-act, fig.cap="Distribution of Number of Reported Activities"}
diaries_long <- mutate(.data = diaries_long, atall = raw > 1)
diaries_long <- group_by(.data = diaries_long, respondent)
n_of_act <- summarise(.data = diaries_long, n_of_act = sum(atall))

g <- ggplot(data = n_of_act, mapping = aes(x = n_of_act)) + geom_histogram(na.rm = TRUE, bins = 17)
g <- g + labs(x = "Number of Activities Rated Often or Sometimes")
if (pensieve:::assert_n_infer_use_js()) {
  ggplotly(g)
} else {
  plot(g)
}
```

As figure \@ref(fig:distro-act) shows, this is by no means an academic problem: Participants differ widely in the number of activities rated at all, from `0` to all `16`, though we do not know what this means.

It appears then, that the BIBB/BAuA data are ill-suited for a Q-mode analysis, at least using conventional (and polychoric) techniques.

The limitations of the activity data in BIBB/BAuA are especially frustrating (see [this issue](https://github.com/QWrks/worktypes/issues/13)), because:

1. Time spent on activities is *clearly* an ipsative phenomenon, participants should have no trouble reporting it as such.
2. The *inverse* problem of analysing ipsative phenomena in R-mode (compositional data) is well understood and largely solved. [^weight-by-hours]

[^weight-by-hours]: In both Q- and R-mode, in addition to the study weight, records could also be weighted by their reported hours.
    Based on the relative wide spread in working hours in figure \@ref(fig:freq-hours), this would change the focus of our analysis considerably; instead of jobs, we would be studying work hours done in the economy.
    See [this issue](https://github.com/QWrks/worktypes/issues/12).


## R-Mode Analysis

### Correlations

We start by inspecting the polychoric correlations, displayed in figure \@ref(fig:polycor).

```{r polycor, fig.cap="Polycholic Correlations With Pairwise Complete Observations (Unweighted)"}
polies <- polycor::hetcor(data = as.data.frame(act[, 1:16]), 
                          ML = TRUE, 
                          std.err = TRUE, 
                          bins = 3, 
                          pd = TRUE,
                          use = "pairwise.complete.obs")
pensieve:::plot_heatmap(color_matrix = polies$correlations, 
                        color_title = "Polychoric Correlations")
```

Surprisingly, there are *some* correlated activities, but none that are especially strongly correlated.

To check the result, we can also inspect weighted polychoric correlations using a different package in figure \@ref(fig:weighted-polycor).

```{r weighted-polycor, fig.cap="Polycholic Correlations With Pairwise Complete Observations (Weighted)"}
act_compl <- na.omit(act)
w_polies <- polies$correlations
w_polies[,] <- NA
for (rw in 1:nrow(w_polies)) {
  for (cl in 1:ncol(w_polies)) {
    w_polies[rw, cl] <- wCorr::weightedCorr(
      x = unlist(act_compl[, rw], use.names = FALSE),
      y = unlist(act_compl[, cl], use.names = FALSE),
      method = "Polychoric",
      weights = act_compl$weight,
      ML = TRUE)
  } 
}
pensieve:::plot_heatmap(color_matrix = w_polies,
                        color_title = "Polychoric Correlations")
```

The differences are neglibible.


### Parallel Analysis

Next, we run a parallel analysis to determine the number of factors that can be exctracted from the data.

```{r paran, eval=FALSE}
random.polychor.pa::random.polychor.pa(data.matrix = as.matrix(act[,1:16]),
                                       nrep = 10,
                                       q.eigen = .95,
                                       comparison = "random",
                                       fit.pa = FALSE,
                                       print.all = FALSE)
help(random.polychor.pa)
```

